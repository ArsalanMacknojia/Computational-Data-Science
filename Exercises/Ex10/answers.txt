// CMPT 353 - Exercise 10
// Arsalan Macknojia

Q. How long did your reddit_averages.py take with (1) the reddit-0 data set and effectively no work, (2) no schema specified and not caching (on reddit-2 for this and the rest), (3) with a schema but not caching, (4) with both a schema and caching the twice-used DataFrame? [The reddit-0 test is effectively measuring the Spark startup time, so we can see how long it takes to do the actual work on reddit-2 in the best/worst cases.]
A.
1. reddit-0 data set and effectively no work
   real 0m17.249s
   user 0m34.616s
   sys  0m2.474s

2. with no schema specified and not caching
   real 3m24.632s
   user 3m51.536s
   sys  0m3.492s

3. with a schema but not caching
   real 0m24.929s
   user 0m44.469s
   sys  0m2.881s

4. with both a schema and caching the twice-used DataFrame
   real 0m22.309s
   user 0m41.667s
   sys  0m2.653s

Q. Based on the above, does it look like most of the time taken to process the reddit-2 data set is in reading the files, or calculating the averages?
A. Based on the result, reading the file takes most of the time.

Q. Where did you use .cache() in your wikipedia_popular.py? [Hint: the answer had better be “once”… but where?]
A. I used .cache() before sorting the dataframe by date/hour and page name.